{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "files = os.listdir(path)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_csv = [f for f in files if f[-4:] == '.csv']\n",
    "files_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files_csv:\n",
    "    data = pd.read_csv(f, 'rawdata')\n",
    "    df = df.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"allData.csv\",usecols=['Description', 'Product', 'Reason For Calling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"Closed Duplicates Cases from Accounts - August 2018 - Sheet1.csv\",usecols=['Description','Problem Category', 'Reason for calling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns=['Description','Problem Category', 'Reason for calling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis = 0, subset=['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimAllColumns(df):\n",
    "    \"\"\"\n",
    "    Trim whitespace from ends of each value across all series in dataframe\n",
    "    \"\"\"\n",
    "    df['Description'] = df.Description.replace(\"\\n\", \" \")\n",
    "    trimStrings = lambda x: x.strip() if type(x) is str else x\n",
    "    return df.applymap(trimStrings)\n",
    "\n",
    "\n",
    "# simple example of trimming whitespace from data elements\n",
    "\n",
    "df = trimAllColumns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#coding:utf-8\n",
    "# Author: Alejandro Nolla - z0mbiehunt3r\n",
    "# Purpose: Example for detecting language using a stopwords based approach\n",
    "# Created: 15/05/13\n",
    "\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from nltk import wordpunct_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "except ImportError:\n",
    "    print ('[!] You need to install nltk (http://nltk.org/index.html)')\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "def _calculate_languages_ratios(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return a dictionary that looks like {'french': 2, 'spanish': 4, 'english': 0}\n",
    "    \n",
    "    @param text: Text whose language want to be detected\n",
    "    @type text: str\n",
    "    \n",
    "    @return: Dictionary with languages and unique stopwords seen in analyzed text\n",
    "    @rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    languages_ratios = {}\n",
    "\n",
    "    '''\n",
    "    nltk.wordpunct_tokenize() splits all punctuations into separate tokens\n",
    "    \n",
    "    >>> wordpunct_tokenize(\"That's thirty minutes away. I'll be there in ten.\")\n",
    "    ['That', \"'\", 's', 'thirty', 'minutes', 'away', '.', 'I', \"'\", 'll', 'be', 'there', 'in', 'ten', '.']\n",
    "    '''\n",
    "\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens]\n",
    "\n",
    "    # Compute per language included in nltk number of unique stopwords appearing in analyzed text\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "\n",
    "        languages_ratios[language] = len(common_elements) # language \"score\"\n",
    "\n",
    "    return languages_ratios\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Calculate probability of given text to be written in several languages and\n",
    "    return the highest scored.\n",
    "    \n",
    "    It uses a stopwords based approach, counting how many unique stopwords\n",
    "    are seen in analyzed text.\n",
    "    \n",
    "    @param text: Text whose language want to be detected\n",
    "    @type text: str\n",
    "    \n",
    "    @return: Most scored language guessed\n",
    "    @rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    ratios = _calculate_languages_ratios(text)\n",
    "\n",
    "    most_rated_language = max(ratios, key=ratios.get)\n",
    "\n",
    "    return most_rated_language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for index, row in df['Description'].iteritems():\n",
    "        #print(row)\n",
    "        if(row!=\" \"):\n",
    "            lang = detect_language(row)\n",
    "            df.loc[index, 'Language'] = lang\n",
    "            #print (lang)\n",
    "            #cleanData = preprocess(row)\n",
    "            #df.loc[index, 'Clean_Desc'] = cleanData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = df[df.Language==\"english\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from email_reply_parser import EmailReplyParser\n",
    "for index, row in df_eng['Description'].iteritems():\n",
    "        if(row!=\" \"):\n",
    "            abc = EmailReplyParser.parse_reply(row)\n",
    "            data = re.sub(r'<.*?>', '', abc)\n",
    "            df_eng.loc[index, 'primaryMail'] = data\n",
    "            print(index)\n",
    "            print(\"**********************************\")\n",
    "            print(data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng.primaryMail.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def checkNa(row):\n",
    "    #print(row['primaryMail'])\n",
    "    #print(row['primaryMail'])\n",
    "\n",
    "    if(row['primaryMail']==0):\n",
    "        print(row['Description'])\n",
    "        soup = BeautifulSoup(row['Description'])\n",
    "        print(soup.get_text())\n",
    "        abc = EmailReplyParser.parse_reply(soup.get_text())\n",
    "        data = re.sub(r'<.*?>', '', abc)\n",
    "        row['primaryMail'] = data\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng.apply(checkNa,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectlang(row):\n",
    "    try:\n",
    "        lang = detect(row['primaryMail'])\n",
    "        print(lang)\n",
    "        row['Language'] = lang\n",
    "    except:\n",
    "        row['Language'] = \"error\"\n",
    "    return row\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = df_eng.apply(detectlang,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng.Language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = df_eng[df_eng.Language==\"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng.Language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng['primaryMail'] = df_eng.primaryMail.replace(\"?\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for index, row in df_eng['primaryMail'].iteritems():\n",
    "    #print(row)\n",
    "    print(index)\n",
    "    if(row!=\" \"):\n",
    "        mailWithOnlyAlphabet = re.sub(r'[^a-zA-Z]', ' ',row)\n",
    "        mail = re.sub(\"\\s\\s+\", \" \", mailWithOnlyAlphabet)\n",
    "        df_eng.loc[index, 'mail'] = mail\n",
    "        df_eng.loc[index, 'length'] = len(mail)\n",
    "        print(\"******************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = trimAllColumns(df_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng.length = df_eng.length.astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = df_eng[df_eng.length>8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng.mail = df_eng.mail.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_set = {'forwarded','message','lz','logitech','dear', 'my', 'date', 'i', 'recently', 'hi', 'hello', 'product', 'serial', 'number', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'purchased', \n",
    "               'purchase', 'support', 'http', 'com', 'logitech', 'www', 'https', 'logi', 'customercare',\n",
    "               'contact', 'terms', 'blvd', 'gateway', 'newark', 'usa', 'logo' ,'care', 'ca', 'footer', 'use', \n",
    "               'customer', 'owned', 'us', 'survey', 'americas', 'copyright', 'headquarters', 'owners', 'respective',\n",
    "               'the','rights', 'trademarks', 'reserved', 'property','dear','regards','thanks', 'mail', 'email','product','serial','number',\n",
    "              'lz','g','x','k','date','like','get','one','set','thank','also','two',\n",
    "              'see','able','n','could',\n",
    "              'since','last','know','still','got','pm','p','n','s'\n",
    "              'operating','system','platform','ce','s','hs'}\n",
    "\n",
    "df_eng['mail_text'] = df_eng['mail'].str.split(' ').apply(lambda x: ' '.join(k for k in x if k not in replace_set))\n",
    "\n",
    "df_eng['mail_text'] = df_eng['mail_text'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLength(row):\n",
    "    row['length']=len(row['mail_text'])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = df_eng.apply(findLength,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = df_eng[df_eng.length>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(row):\n",
    "    sentence = row['mail_text']\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    row['clean'] = tokens\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = df_eng.apply(preprocess,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLength(row):\n",
    "    row['length']=len(row['clean'])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = df_eng.apply(findLength,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = df_eng.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng.to_csv(\"full_preprocessedData_4-10-2018.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
